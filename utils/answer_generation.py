import logging
from typing import Union
from pydantic import BaseModel, Field

from utils.config import *
from utils.utils import *
from utils.utils import get_local_data

logger = logging.getLogger(__name__)

date, day = get_local_data()
place = 'Bangalore, India'

def query_agent(query, llm, date, day):
    """
    Generates a list of subqueries based on a given query using an LLM agent.

    Args:
        query (str): The initial query.
        llm: The language model instance used for generating content.
        date (str): The current date.
        day (str): The current day of the week.

    Returns:
        list: A list of subqueries generated by the LLM, cleaned and formatted.
    """

    class SearchSubqueries(BaseModel):
        """Generates a list of subqueries based on a given query using an LLM agent.
        - Use date/location only if the query requires time-sensitive or location-specific information.
        Replace time-specific words with context details.
        Rephrase the query for search engine optimization and clarity.
        Avoid including the date if not necessary.
        Emphasize date or location when needed.
        When breaking down into multiple tasks, always consider the overall task to ensure every aspect is included. By the end of searching for subqueries, there should be enough information to process the answer.
        If necessary, split the query into up to 2 diverse subqueries to cover all aspects.
        Subqueries should not be in a numbered list.
        Plan searches to answer every part of the query without overemphasizing any term.
        Ensure that subqueries include essential keywords and context from the original query to maintain relevance.
        Avoid generating subqueries that are too generic or lack specific terms from the original query.
        When rephrasing, retain the main subject and important details from the original query in each subquery.
        """ + "Todays date is: {date} and today is {day}, {place}, Use date/location only if the query requires time-sensitive or location-specific information.",

        planning_to_answer_query_to_help_finding_subqueries: list[str] = Field(
            description="Up to 6 planning steps breaking down the task without overemphasizing any keyword, Context: Today's date is {date} {day}, {place}."
        )
        subqueries: list[str] = Field(
            description="Up to 6 rephrased phrases covering all planning steps in independent search phrases , using a maximum of 4 words per subquery, avoiding unnecessary adjectives, Context: Today's date is {date} {day}, {place}"
        )
        is_summary: bool = Field(
            description="Whether the subqueries are intended to summarize."
        )
        is_covered_urls: bool = Field(  
            description="Whether the query is asking something that is related to youtube or reddit."
        )

    logger = logging.getLogger(__name__)
    try:
        response = llm.with_structured_output(SearchSubqueries).invoke(
            f"Todays date is: {date} and today is {day}, {place}, Use date/location only if the query requires time-sensitive or location-specific information. "
            f"\nquery: {query}"
        )
        is_summary = response.is_summary
        is_covered_urls = response.is_covered_urls
        response = response.subqueries
    except Exception as e:
        logger.warning(f"Structured output failed: {e}. Falling back to prompt-based extraction.")
        try:
            prompt = prompts['query_agent_basic'].format(date=date, day=day, query=query)
            response = llm.invoke(prompt).content
            response = extract_subqueries(response.text)
        except Exception as ex:
            logger.error(f"Both structured and prompt-based extraction failed: {ex}")
            return []
    try:
        response = [
            r.lower().replace('*', '').replace("subquery", "").replace("'", "")
            for r in response if r != ''
        ]
    except Exception as e:
        logger.error(f"Error cleaning subqueries: {e}")
        return []
    return response, is_summary,is_covered_urls


def response_gen(model, query, context):
    """
    Generates a comprehensive response for a given query using an LLM, incorporating context.

    Args:
        model: The language model instance used for generating content.
        query (str): The query for which a response is needed.
        context (str): The context to be included in the response generation.

    Returns:
        tuple: The synthesized answer and sources formatted in markdown.
    """

    class ResponseGen(BaseModel):
        f"""Generates a comprehensive response for a given query using an LLM, incorporating context.
        Instructions:

        Understand the query thoroughly, even if it contains misspellings or errors. Use the provided context and related information to interpret the intended meaning.

        Answer the query using the context and available information; if the context is insufficient, utilise whatever information is relevant, its fine even if answer is incomplete. utilize your own knowledge to provide a complete answer, including code if requested.

        Provide direct answers: If the query asks for code, supply the code rather than a plan to implement it. Learn facts, syntax, and structures from the context to create accurate responses.

        Enhance your response with engaging details, clear reasoning, and simple explanations. Use analogies to illustrate difficult concepts when appropriate.

        Cite sources for every detail using hyperlinks to the search result URLs; if using your own knowledge, cite as "llm_generated". Structure your answer in markdown format with bullet points. Utilize the context fully, even if it doesn't directly answer the question. Mention any next steps in next_steps; otherwise, write None.

        Query:
        {query}

        Context:
        {context}

        KEEP ANSWER CONCISE UNLESS ASKED FOR DETAILED.
        """
        intent_understanding: str = Field(
            description="Understanding of the query, including any misspellings or errors."
        )
        synthesized_answer_based_on_various_sources: str = Field(
            description="Concise answer to the query, using all available information, even if limited."
        )
        sources: list[str] = Field(
            description="List of sources used to answer the query, including hyperlinks to search results."
        )
        next_steps: Union[str, None] = Field(
            description="Next steps/searches needed based on the query, if applicable; otherwise, None."
        )

    logger.info("Generating Answer for query: %s", query)

    try:
        response = model.with_structured_output(ResponseGen).invoke(
            f"Query: {query}\nContext: {context}"
        )
        response = response.dict()
        sources = '\n'.join([f'[{i}]. ' + s + '\n' for i, s in enumerate(response['sources'])])
        answer = (
            "#### Answer: \n"
            + str(response['synthesized_answer_based_on_various_sources'])
            + "\n\n#### Next steps:\n"
            + str(response['next_steps'])
        )
        logger.info("Structured response generated successfully.")
    except Exception as e:
        logger.warning(f"Structured output failed: {e}. Falling back to prompt-based generation.")
        try:
            prompt = prompts['qa_response_generation'].format(context=context, query=query)
            prompt_template = f"{prompt}"
            response = model.invoke(prompt_template).content
            answer = "#### Answer: \n" + response
            sources = ''
            logger.info("Fallback prompt-based response generated successfully.")
        except Exception as ex:
            logger.error(f"Both structured and prompt-based response generation failed: {ex}")
            return "Error: Unable to generate answer.", ""
    return answer, sources


def summarizer(query, docs, llm, batch):
    """
    Summarizes a list of documents iteratively in batches using an LLM.

    Args:
        query (str): The initial query guiding the summarization.
        docs (list): A list of documents to be summarized.
        llm: The language model instance used for generating content.
        batch (int): The number of documents to process in each batch.

    Returns:
        list: A summarized version of the input documents.
    """

    if not docs or not isinstance(docs, list):
        logger.warning("No documents provided or docs is not a list.")
        return []

    len_docs = len(docs)
    if len_docs == 1:
        logger.info("Only one document provided, summarizing it.")
        try:
            comb_docs = f'Document 0: {str(docs[0])}'
            prompt = prompts['summary_generation'].format(comb_docs=comb_docs, query=query)
            response = llm.invoke(prompt).content
            summary_text = getattr(response, 'text', None) or getattr(response, 'content', None) or str(response)
            return [summary_text.strip()] if summary_text else []
        except Exception as e:
            logger.error(f"Summarization failed for single document: {e}")
            return []

    print(f"Summarising using {len_docs} documents")
    while len_docs > 1:
        summaries = []
        for i in range(0, len_docs, batch):
            batch_docs = docs[i:i + batch]
            if not batch_docs:
                continue
            comb_docs = '\n'.join(
                [f'Document {j+i}:' + str(d) for j, d in enumerate(batch_docs)]
            )
            try:
                prompt = prompts['summary_generation'].format(comb_docs=comb_docs, query=query)
                response = llm.invoke(prompt).content
                # Handle different LLM output structures
                summary_text = getattr(response, 'text', None) or getattr(response, 'content', None) or str(response)
                if summary_text:
                    summaries.append(summary_text.strip())
                else:
                    logger.warning(f"Empty summary for batch starting at {i}.")
            except Exception as e:
                logger.error(f"Summarization failed for batch starting at {i}: {e}")
                continue

        if not summaries:
            logger.error("No summaries generated in this iteration, aborting.")
            return []

        len_docs = len(summaries)
        docs = summaries

    return '\n'.join(docs)
